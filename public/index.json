[{"content":"Advanced Text Analysis and Generation Project Project Overview Disclaimer: This project is still under construction. More updates will be added to this page later. If you want to contribute, ask, or talk about it, feel free to reach out!\nThis project is a sophisticated demonstration of advanced Natural Language Processing (NLP) and deep learning techniques. The goal is to create a comprehensive text analysis and generation system that leverages state-of-the-art language models and cutting-edge methodologies. This project exemplifies the ability to handle complex data preprocessing, model training, fine-tuning, hyperparameter optimization, model evaluation, and deployment processes.\nObjectives Data Collection: Aggregate and process large-scale, diverse text datasets to create a rich training corpus. Data Preprocessing: Implement advanced techniques for text cleaning, normalization, tokenization, and augmentation to prepare high-quality input data. Model Training: Utilize state-of-the-art language models and advanced training techniques to build robust NLP systems. Hyperparameter Tuning: Conduct rigorous hyperparameter optimization to enhance model performance and efficiency. Model Evaluation: Apply robust evaluation metrics to assess model performance comprehensively. Model Interpretability: Use interpretability tools to gain insights into model behavior and ensure transparency. Deployment: Deploy the trained model using modern deployment platforms to enable real-time inference and interaction. Technologies Used Python: The primary programming language for implementing all project components. Hugging Face Transformers: A powerful library for accessing and utilizing state-of-the-art NLP models. Datasets Library: Facilitates efficient handling and processing of large-scale datasets. SentencePiece: Tokenizer and text processor for handling subword units, enhancing model\u0026rsquo;s ability to understand text. Optuna: A hyperparameter optimization framework that systematically improves model performance. SHAP (SHapley Additive exPlanations): A tool for interpreting machine learning models and visualizing their predictions. Gradio: A library for creating interactive user interfaces for machine learning models, simplifying deployment and usage. scikit-learn: Utilized for data splitting, evaluation metrics, and other utility functions. Models Used T5 (Text-To-Text Transfer Transformer): A versatile model from Hugging Face capable of performing various NLP tasks by framing them as text-to-text problems. BERT (Bidirectional Encoder Representations from Transformers): A pre-trained model from Hugging Face used for understanding the context within text, essential for tasks like text classification and entity recognition. GPT-4 (Generative Pre-trained Transformer 4): An advanced model used for text generation tasks, capable of producing human-like text based on the given input. Datasets Common Crawl: A vast and diverse web corpus that provides extensive training data, ensuring the model learns from a wide variety of text. Custom Web Scraped Data: Additional data collected through web scraping from multiple websites to enrich the training dataset and improve model robustness. Methodologies Data Preprocessing Text Cleaning: Implementing techniques to remove HTML tags, special characters, and normalize text to lower case, ensuring data consistency. Data Augmentation: Employing methods such as back-translation (translating text to another language and back), synonym replacement, and noise injection to enhance data diversity and model generalization. Tokenization: Using SentencePiece for subword tokenization, enabling better handling of rare words and improving the model\u0026rsquo;s understanding of text. Model Training and Fine-Tuning Mixed Precision Training: Utilizing half-precision floating-point format to speed up training and reduce memory usage, facilitated by frameworks like PyTorch. Distributed Training: Leveraging multiple GPUs to parallelize training, significantly reducing training time and improving efficiency. Hyperparameter Tuning Optuna: Conducting hyperparameter optimization using Optuna, systematically exploring the hyperparameter space to identify the optimal settings for learning rate, batch size, and epochs. Model Evaluation BLEU (Bilingual Evaluation Understudy): A metric for evaluating the quality of text generation, especially useful in translation tasks. ROUGE (Recall-Oriented Understudy for Gisting Evaluation): A set of metrics for evaluating summarization and text generation by comparing overlaps with reference outputs. Model Interpretability SHAP: Using SHAP to interpret model predictions, providing insights into the contribution of each feature to the output, ensuring model transparency and trustworthiness. Deployment Gradio: Creating an interactive web interface using Gradio, enabling users to interact with the model in real-time, generating text based on user input and displaying results instantly. Conclusion This advanced text analysis and generation project integrates the latest models, cutting-edge technologies, and sophisticated methodologies to demonstrate expertise in NLP and deep learning. By showcasing capabilities in data collection, preprocessing, model training, hyperparameter optimization, evaluation, and deployment, this project exemplifies a comprehensive and professional approach to building state-of-the-art NLP systems. The use of open-source tools and models ensures accessibility and reproducibility, making it a valuable addition to any AI portfolio.\n","permalink":"https://abirarsalane.github.io/projects/advanced-current-project/","summary":"Advanced Text Analysis and Generation Project Project Overview Disclaimer: This project is still under construction. More updates will be added to this page later. If you want to contribute, ask, or talk about it, feel free to reach out!\nThis project is a sophisticated demonstration of advanced Natural Language Processing (NLP) and deep learning techniques. The goal is to create a comprehensive text analysis and generation system that leverages state-of-the-art language models and cutting-edge methodologies.","title":"Advanced Text Analysis and Generation Project"},{"content":"Introduction I am proud to be a member of Atlasia the Moroccan community of engineers, researchers, professors, students, and professionals dedicated to building the next generation of customized, open AI models. Our mission is to develop AI technologies that are aligned with our values, identity, and culture. This open-source initiative focuses on creating models, datasets, benchmarks, and other resources to advance the field of artificial intelligence in Morocco.\nEnglish-to-Darija Translation Models Project Goals The primary goal of the English-to-Darija translation project was to create high-quality models capable of translating English text into Darija, the colloquial Arabic spoken in Morocco. This initiative aimed to:\nPreserve and promote the Darija language by making it more accessible in the digital age. Help moroccans and non-moroccans translate from darija to english and vice-versa. Provide a valuable resource for the Moroccan diaspora and non-Darija speakers interested in learning the language. Facilitate better communication and cultural exchange between English and Darija speakers. Development Process Data Collection: Gathered a diverse and extensive dataset of parallel English-Darija texts by the help of volunteers through a website where they can input data in a user friendly interface, and generated datasets from the team and all open-sources contributors.\nData Preprocessing: The collected data was cleaned, normalized, and annotated to ensure high quality and consistency. This step was crucial for training effective machine learning models.\nModel Training: Trained several translation models using state-of-the-art neural network architectures. Experimented with different hyperparameters and configurations to optimize performance.\nEvaluation and Benchmarking: The models were rigorously evaluated using standard metrics and benchmarks. Compared their performance against existing translation tools to ensure they met our quality standards.\nOpen-Sourcing: Once the models were finalized, they were published on Hugging Face, making them freely available to the global community. This aligns with our commitment to open-source principles and collaborative development.\nThe current available models are:\nTerjman Nano (77M parameters) Terjman Large (240M parameters) Terjman Ultra (1.3B parameters) Terjman Supreme (3.3B parameters) Closer look at Terjman Suprem: The model is built upon the powerful Transformer architecture, leveraging state-of-the-art natural language processing techniques. It is a fine-tuned version of facebook/nllb-200-3.3B on a the darija_english dataset enhanced with curated corpora ensuring high-quality and accurate translations.\nIt achieves the following results on the evaluation set:\nLoss: 2.3687 Bleu: 5.6718 Gen Len: 39.9504 The finetuning was conducted using a A100-40GB and took 57 hours.\nThe current available datasets are:\ndarija_english ATAM DODa Impact and Future Work The release of the English-to-Darija translation models has had a significant impact:\nAccessibility: These models have made it easier for people to access and understand Darija, bridging the language gap between English and Darija speakers. Cultural Preservation: By promoting the use of Darija in digital platforms, we are helping to preserve and celebrate Moroccan culture. Educational Tool: These models serve as a valuable resource for educators and students, enhancing language learning and research. Future Plans Building on this success, the community plans to:\nExpand the Dataset: Continuously update and expand the dataset to improve model accuracy and coverage. Develop Additional Models: Create more AI models for other Moroccan languages and dialects. Conclusion Joining and contributing to the Moroccan AI community has been an enriching experience. Together, we are not only advancing AI technology but also fostering a sense of pride and identity through our work. I look forward to continuing this journey and making further contributions to this vibrant and impactful community.\n","permalink":"https://abirarsalane.github.io/blog/atlasia/","summary":"Introduction I am proud to be a member of Atlasia the Moroccan community of engineers, researchers, professors, students, and professionals dedicated to building the next generation of customized, open AI models. Our mission is to develop AI technologies that are aligned with our values, identity, and culture. This open-source initiative focuses on creating models, datasets, benchmarks, and other resources to advance the field of artificial intelligence in Morocco.\nEnglish-to-Darija Translation Models Project Goals The primary goal of the English-to-Darija translation project was to create high-quality models capable of translating English text into Darija, the colloquial Arabic spoken in Morocco.","title":"Open Source Moroccan LLMs project"},{"content":"Introduction The project Scia-Tech is a project combining the power of two powerful technologies: AI and IOT. Scia-Tech\u0026rsquo;s smart insoles are designed to enhance the user\u0026rsquo;s daily life. These smart insoles not only track your physical activities, but also offer a unique solution for individuals suffering from sciatica. By closely monitoring their walking patterns, sitting habits, heavy lifting, intense workouts, posture, and any movement. Scia-Tech aims to identify behaviors that may trigger sciatica pain, allowing you to take proactive steps to manage and mitigate discomfort.\nDeeper dive Scia-Tech offers:\nSensor Technology: It provides cutting-edge sensor technology to revolutionize pain management and improve health. Data Analysis and Machine Learning: Collected data from the sensors is processed and the models predict the movement patterns and postures, then this output is fed to other models to forecast potential pain triggers before they strike. The recommendation system then provides personalized insights, guiding the user towards a life free from discomfort. Personal Space: All of this information and more is sent to the user\u0026rsquo;s account on the mobile application. The user can find reports, get notifications, report pain manually, manage their profile, see the timer predicting when sciatica pain may occur. Technical Aspect The ESP32, equipped with our sensors, sends real-time data to AWS IoT Core using MQTT protocol.\nThis raw data undergoes transformation using ML and DL models to predict position,posture and expected sciatica pain onset.\nWith personal data from the mobile app (from the user’s account including age, gender, weight, height..), real-time pain reports (from the user reporting they’re currently experiencing sciatica pain), and continuous IoT data streaming (used to predict the posture, position, movement and any excessive weight lifted), we create personalized AI-generated recommendations crafted specifically to you.\nLet\u0026rsquo;s break it down - Scia-Tech\u0026rsquo;s system, powered by cloud-based microservices, efficiently manages batch oriented workflows and streaming data to process it and generate the adequate outputs.\nPersonalized recommendations mean the system learns from the user\u0026rsquo;s unique data, not only giving you tailored guidance but also improving our general AI recommendation system.\n1. Input Data From the mobile application: When creating their account, the user enters the following information that we take into consideration when training the models. These inputs are: Age, Gender, Weight, Height.\nFrom the sensors: The insoles are equipped with different sensors including: ESP32 microcontroller,FSR01 load cell and MPU6050 Sensor,\nMPU: Sx, Sy, Sz, Qx, Qy, Qz, Ww, FPS (for each feet) Pressure: P0, P1, P2, P3, P4 (for each feet) 2. How it works Data Generation and Research Having worked on an innovative project, finding data to train the models was pretty challenging. I couln\u0026rsquo;t find any data for movement, activity or posture prediction from sensors installed in insoles. I couldn\u0026rsquo;t even find data collected the sensors we had combined. The unique combination of sensors in insoles to predict pain made finding the data a real obstacle, especially free and open source data. Collecting data from hospitals wasn;t an option either because it\u0026rsquo;s confidential. The solution was generating my own dataset and that\u0026rsquo;s exactly was I did. I generated a dataset of douzains of people using the insoles in different positions and postures, doing various activities such as walking, sitting, running, jumping, lifting heavy weights,\u0026hellip; I worked alongside a doctor, I got all the information necessary and, throught weekly meetings, kept iterating my models, feature engineering and experimenting to increase the performance. The results can never be perfect because in the medical field it\u0026rsquo;s very hard to predict the future because nature cannot be predicted, people are all different and unique and no case is like the other. The goal was not to get perfect results but to increase the probablities accuracy based on all the elements we could have. More data could\u0026rsquo;ve been beneficial but the goal was to implement the project using insoles and insoles only because that was our innovative approach. This project required a lot of research, from domain research about sciatica, medical conditions, exceptions, and parameters, to technical research when deciding on the right models, the right features, reading similar research papers and more.\nThe pipeline The ESP32, equipped with our sensors, sends real-time data to AWS IoT Core using MQTT protocol. This raw data undergoes transformation first using AWS Lambda functions for pre-processing tasks, distribution of inputs into different databases such as AWS S3 and AWS DynamoDB, and posture and movement prediction using an ML pre-trained model stored in a Lambda function. The processed data is then ingested as an input of the sciatica pain predictive model using AWS SageMaker. The outputs are then sent to our mobile application for display.\n3. Output Data The output of this project is:\nPain Prediction: The app features AI technology integrated with smart insole data to display a timer predicting when sciatica pain may occur. You can also manually log pain incidents, specifying whether they occurred during sitting, standing, or heavy activity. Reports: Access daily, weekly, and historical reports of your activities and pain incidents. Notifications: Receive reminders and notifications to help you manage your daily routines and avoid pain triggers. Profile Management: Update and modify your profile information as needed. ","permalink":"https://abirarsalane.github.io/projects/sciatech/","summary":"Introduction The project Scia-Tech is a project combining the power of two powerful technologies: AI and IOT. Scia-Tech\u0026rsquo;s smart insoles are designed to enhance the user\u0026rsquo;s daily life. These smart insoles not only track your physical activities, but also offer a unique solution for individuals suffering from sciatica. By closely monitoring their walking patterns, sitting habits, heavy lifting, intense workouts, posture, and any movement. Scia-Tech aims to identify behaviors that may trigger sciatica pain, allowing you to take proactive steps to manage and mitigate discomfort.","title":"AI connected medical insoles"},{"content":"Introduction I\u0026rsquo;m happy to share the latest project I\u0026rsquo;ve worked on \u0026lsquo;EcoTeach\u0026rsquo;. The project\u0026rsquo;s focus is clear: environmental education for children. I have developped a speech-to-speech chatbot that would empower young minds with knowledge about climate change. Why kids? Because they are like sponges, eagerly absorbing information and forming lifelong habits.\nTechnical Marvels: Models, Frameworks, and Fine-Tuning 1. Models from Hugging Face We delved into the rich repository of Hugging Face models to find the best models for our use case. After benchmarking a couple of them in the limited amount of time we had, we went with models that balanced accuracy, fluency, and computational efficiency:\nSTT: facebook/seamless_m4t: foundational multilingual and multitask model that seamlessly translates and transcribes speech and text across 100 languages (including Moroccan dialect) LLM: mistralai/Mistral-7B-Instruct-v0.1: The SOTA 7B LLM at the time of the hackathon. TTS: LeeSangHoon/HierSpeech_TTS: Speech synthesis at its finest! We harnessed this model to create expressive and context-aware speech utterances. 2. Langchain Framework To orchestrate our chatbot\u0026rsquo;s magic, we turned to the Langchain framework. Its modular design allowed us to seamlessly integrate different components. We crafted a pipeline that combined text preprocessing, model inference, and speech synthesis. The result? A harmonious symphony of words and voices.\n3. Fine-Tuning and Prompt Engineering Our chatbot wasn\u0026rsquo;t just an out-of-the-box solution. We fine-tuned our chosen models using PEFT library on domain-specific data related to climate change. By feeding them relevant prompts, we tailored their responses to environmental queries. The art of prompt engineering became our secret sauce, ensuring that our chatbot spoke the language of eco-consciousness.\nStreamlit: The Interactive Interface A chatbot without a user-friendly interface is like a book without a cover. Enter Streamlit! We crafted an elegant web app that allowed users to converse with our chatbot effortlessly. The interface featured input fields where curious minds could ask questions about climate, pollution, and sustainability. The chatbot responded with synthesized speech, making learning engaging and accessible.\nSo, next time you see a child interacting with our chatbot, remember: we\u0026rsquo;re nurturing a greener future, one conversation at a time.\n","permalink":"https://abirarsalane.github.io/projects/ecoteach/","summary":"Introduction I\u0026rsquo;m happy to share the latest project I\u0026rsquo;ve worked on \u0026lsquo;EcoTeach\u0026rsquo;. The project\u0026rsquo;s focus is clear: environmental education for children. I have developped a speech-to-speech chatbot that would empower young minds with knowledge about climate change. Why kids? Because they are like sponges, eagerly absorbing information and forming lifelong habits.\nTechnical Marvels: Models, Frameworks, and Fine-Tuning 1. Models from Hugging Face We delved into the rich repository of Hugging Face models to find the best models for our use case.","title":"Building a Speech-to-Speech Chatbot for enviromental Education"},{"content":"Introduction For the past few days, I had the pleasure to participate in HIGH-LEVEL FORUM ON ARTIFICIAL INTELLIGENCE (AI) : AI AS A LEVER FOR DEVELOPMENT IN AFRICA, the first high-level forum for artificial intelligence in Africa and the biggest worldwide, held in Rabat, Morocco, and organized by Ai Movement and UM6P - Mohammed VI Polytechnic University, in collaboration with UNESCO.\nWhat is the High-Level Forum on Artificial Intelligence This groundbreaking event brought together a diverse and distinguished group of attendees, including the Senior Adviser of His Majesty King Mohammed VI ,former and current ministers, UNESCO prominent members, numerous chiefs, directors, artists, founders, CEOs, consultants, presidents, organization directors, department heads, ambassadors, professors, among many others.\nThe panels were nothing short of inspiring, filled with groundbreaking insights and visionary ideas. Engaging with speakers during and after the sessions was a highlight- so many brilliant minds coming together to shape the future of AI in Africa.\nThe impact Meeting such a diverse group of passionate professionals was energizing. The conversations were rich, the ideas were bold, the optimism was contagious, and the knowledge was infectious. I left feeling more inspired than ever about the potential of AI to transform our continent.\nThe event not only showcased the current advancements but also highlighted the collaborative spirit necessary to drive innovation forward. The shared commitment to leveraging AI for the betterment of our society was truly uplifting. I am confident that with continued collaboration and dedication, we can achieve remarkable progress in the AI landscape of our country and continent.\nIf you want more information about this event, visit their website .\nA heartfelt thank you to everyone who made this event possible, and to everyone I had the chance to meet and talk with. It was a real honor! Here’s to a bright and innovative future for AI in Africa!\n","permalink":"https://abirarsalane.github.io/blog/high-level-forum-ai/","summary":"Introduction For the past few days, I had the pleasure to participate in HIGH-LEVEL FORUM ON ARTIFICIAL INTELLIGENCE (AI) : AI AS A LEVER FOR DEVELOPMENT IN AFRICA, the first high-level forum for artificial intelligence in Africa and the biggest worldwide, held in Rabat, Morocco, and organized by Ai Movement and UM6P - Mohammed VI Polytechnic University, in collaboration with UNESCO.\nWhat is the High-Level Forum on Artificial Intelligence This groundbreaking event brought together a diverse and distinguished group of attendees, including the Senior Adviser of His Majesty King Mohammed VI ,former and current ministers, UNESCO prominent members, numerous chiefs, directors, artists, founders, CEOs, consultants, presidents, organization directors, department heads, ambassadors, professors, among many others.","title":"High-Level Forum on Artificial Intelligence"},{"content":"OCI Generative AI Professional Certification The Oracle Cloud Infrastructure 2024 Generative AI Professional course caters to Software Developers, Machine Learning/AI Engineers, and Gen AI Professionals. It offers an in-depth technical introduction to Large Language Models (LLMs), covering various aspects such as LLM architecture, fine-tuning techniques, code models, multimodal LLMs, and language agents. Additionally, the course covers topics like pretrained foundational models, summarization, embeddings, Dedicated AI Clusters, and the OCI Generative AI security architecture.Throughout the course, participants will engage in practical exercises, including building a conversational chatbot using the OCI Generative AI service. They will learn to enhance the chatbot\u0026rsquo;s conversational abilities by integrating memory and implementing RAG with LangChain. Finally, students will gain experience deploying the chatbot on an OCI compute instance.\nWhat I learnt in this journey Fundamentals of Large Language Models (LLMs): LLM basics, LLM architectures, Prompt Engineering, Fine-tuning techniques, fundamentals of code models, Multi-modal LLMs and Language Agents. OCI Generative AI Deep-Dive: Pretrained Foundational Models (Generation, Summarization, Embedding), Flexible Fine-tuning including T-Few technique, Model Inference, Dedicated AI Clusters, Generative AI Security architecture. Build a Conversational Chatbot with OCI Generative AI: Understand RAG, Vector Databases, Semantic Search, build chatbot using LangChain Framework (Prompts, Models, Memory, Chains), Trace and Evaluate chatbot and deploy on OCI. ","permalink":"https://abirarsalane.github.io/blog/oci-certification/","summary":"OCI Generative AI Professional Certification The Oracle Cloud Infrastructure 2024 Generative AI Professional course caters to Software Developers, Machine Learning/AI Engineers, and Gen AI Professionals. It offers an in-depth technical introduction to Large Language Models (LLMs), covering various aspects such as LLM architecture, fine-tuning techniques, code models, multimodal LLMs, and language agents. Additionally, the course covers topics like pretrained foundational models, summarization, embeddings, Dedicated AI Clusters, and the OCI Generative AI security architecture.","title":"OCI Generative AI Professional Certification"},{"content":"Introduction I\u0026rsquo;m thrilled to present my latest project, Multimodal Generative AI. This project combines advanced AI technologies to create a versatile application that showcases the power of modern generative and recognition systems. It integrates text generation, image recognition, and speech processing into a cohesive application, leveraging state-of-the-art models and frameworks.\nTechnical Marvels: Models, Frameworks, and Fine-Tuning 1. Models from Hugging Face and PyTorch We explored a variety of models to identify the best fits for our application. Our selection balances accuracy, fluency, and computational efficiency:\nText Generation: We used the EleutherAI/gpt-neo-2.7B model for generating coherent and contextually relevant text based on a given prompt. Image Recognition: For classifying images, we leveraged the pre-trained EfficientNet-B0 model from PyTorch, known for its efficiency and accuracy. Speech to Text: Utilizing the SpeechRecognition library, we implemented speech-to-text capabilities to convert spoken language into written text. Text to Speech: The gTTS (Google Text-to-Speech) library was used to convert written text into spoken audio, ensuring natural and expressive speech synthesis. 2. FastAPI Framework To build a robust and scalable backend, we employed the FastAPI framework. Its fast performance and intuitive design allowed us to create an API that efficiently handles requests for text generation, image recognition, and speech processing.\n3. Gradio Interface For a user-friendly interface, we turned to Gradio. This framework enabled us to build an interactive web application where users can easily interact with our multimodal AI system. Gradio\u0026rsquo;s simplicity and flexibility made it the perfect choice for developing a seamless user experience.\nModel Integration and Workflow Our application seamlessly integrates different AI models and workflows to provide a comprehensive user experience:\nText Generation: Users input a prompt, and the GPT-Neo model generates a detailed and contextually appropriate response. Image Recognition: Users upload an image, which is then processed by the EfficientNet-B0 model to provide accurate classification results. Speech to Text: Users upload an audio file, and our system transcribes the speech into text using the SpeechRecognition library. Text to Speech: Users provide text input, and our application converts it into spoken audio using gTTS. Interactive Interface: Gradio A powerful application needs a powerful interface. We crafted an intuitive web app with Gradio that allows users to effortlessly interact with our multimodal AI system. The interface features input fields for text, image, and audio uploads, providing a smooth and engaging user experience.\nText Generation Interface: Users enter a prompt and receive a generated text response. Image Recognition Interface: Users upload an image and receive classification results. Speech to Text Interface: Users upload an audio file and receive the transcribed text. Text to Speech Interface: Users enter text and receive synthesized speech output. Conclusion The Multimodal Generative AI project demonstrates the integration of multiple advanced AI technologies into a cohesive application, providing functionalities like text generation, image recognition, and speech processing. By leveraging modern frameworks and libraries, this project showcases the capabilities of generative and recognition systems, making it a comprehensive and advanced AI project.\n","permalink":"https://abirarsalane.github.io/projects/multimodal/","summary":"Introduction I\u0026rsquo;m thrilled to present my latest project, Multimodal Generative AI. This project combines advanced AI technologies to create a versatile application that showcases the power of modern generative and recognition systems. It integrates text generation, image recognition, and speech processing into a cohesive application, leveraging state-of-the-art models and frameworks.\nTechnical Marvels: Models, Frameworks, and Fine-Tuning 1. Models from Hugging Face and PyTorch We explored a variety of models to identify the best fits for our application.","title":"Building a Multimodal Generative AI Assistant"},{"content":"Introduction On December 2023, I embarked on an exhilarating journey during the MoroccoAI Generative AI Hackathon. Organized by MoroccoAI. Our mission? To leverage the latest GenAI advances to address real-world challenges in specific industries.\nThe Challenge: Educating Children on Climate Change Our team\u0026rsquo;s focus was clear: environmental education for children. We envisioned a speech-to-speech chatbot that would empower young minds with knowledge about climate change. Why kids? Because they are like sponges, eagerly absorbing information and forming lifelong habits.\nTechnical Marvels: Models, Frameworks, and Fine-Tuning 1. Models from Hugging Face We delved into the rich repository of Hugging Face models to find the best models for our use case. After benchmarking a couple of them in the limited amount of time we had, we went with models that balanced accuracy, fluency, and computational efficiency:\nSTT: facebook/seamless_m4t: foundational multilingual and multitask model that seamlessly translates and transcribes speech and text across 100 languages (including Moroccan dialect) LLM: mistralai/Mistral-7B-Instruct-v0.1: The SOTA 7B LLM at the time of the hackathon. TTS: LeeSangHoon/HierSpeech_TTS: Speech synthesis at its finest! We harnessed this model to create expressive and context-aware speech utterances. 2. Langchain Framework To orchestrate our chatbot\u0026rsquo;s magic, we turned to the Langchain framework. Its modular design allowed us to seamlessly integrate different components. We crafted a pipeline that combined text preprocessing, model inference, and speech synthesis. The result? A harmonious symphony of words and voices.\n3. Fine-Tuning and Prompt Engineering Our chatbot wasn\u0026rsquo;t just an out-of-the-box solution. We fine-tuned our chosen models using PEFT library on domain-specific data related to climate change. By feeding them relevant prompts, we tailored their responses to environmental queries. The art of prompt engineering became our secret sauce, ensuring that our chatbot spoke the language of eco-consciousness.\nStreamlit: The Interactive Interface A chatbot without a user-friendly interface is like a book without a cover. Enter Streamlit! We crafted an elegant web app that allowed users to converse with our chatbot effortlessly. The interface featured input fields where curious minds could ask questions about climate, pollution, and sustainability. The chatbot responded with synthesized speech, making learning engaging and accessible.\nSo, next time you see a child interacting with our chatbot, remember: we\u0026rsquo;re nurturing a greener future, one conversation at a time.\n","permalink":"https://abirarsalane.github.io/blog/moroccoai-genai-hackathon/","summary":"Introduction On December 2023, I embarked on an exhilarating journey during the MoroccoAI Generative AI Hackathon. Organized by MoroccoAI. Our mission? To leverage the latest GenAI advances to address real-world challenges in specific industries.\nThe Challenge: Educating Children on Climate Change Our team\u0026rsquo;s focus was clear: environmental education for children. We envisioned a speech-to-speech chatbot that would empower young minds with knowledge about climate change. Why kids? Because they are like sponges, eagerly absorbing information and forming lifelong habits.","title":"Building a Speech-to-Speech Chatbot for enviromental Education"},{"content":"Innovation Meets Impact: My Journey with Orange Summer Challenge 2023 Can you imagine a day where innovation meets social impact? I had the privilege of living it on October 17th, 2023, as we celebrated the culmination of the Orange Summer Challenge 2023. This prestigious program brought together brilliant minds from 15 countries across the Orange Digital Centers network, uniting our collective vision to shape the future. 🌍\nThis year, the Orange Summer Challenge revolved around the theme \u0026ldquo;Artificial Intelligence and Internet of Things: A Winning Duo to Tackle the Challenges of Tomorrow!\u0026rdquo; It was an extraordinary opportunity to dive into cutting-edge technologies and harness their potential to make a lasting impact.\nThroughout this transformative journey, we were fortunate to be supported by a strong system provided by Orange , Amazon Web Services (AWS) and EY . The guidance, mentorship, and personalized training we received were instrumental in our growth and development. Our mission was clear: to foster innovation and create meaningful solutions in crucial areas.\nBut what truly made this experience exceptional was the amazing team I had the honor of working with. Their talent, ambition, and dedication were the driving forces behind our success. ✨\nThe pitch At the heart of the challenge, I had the privilege of representing my team and pitching our project. Months of hard work, innovation, and unwavering dedication culminated in the unveiling of our project - Scia-tech, a revolutionary smart insole designed to seamlessly integrate into daily life.\nThe project Picture this: smart insoles that not only monitor your every move but also provide a unique solution for those dealing with sciatica. Our insoles go beyond tracking; they analyze your posture and behaviors to identify patterns that could lead to sciatica pain. What sets us apart is a cutting-edge recommendation system offering a personalized and unique user experience through a mobile app.\nWhy it matters Numbers don\u0026rsquo;t lie:\n+70% PATIENTS live with sciatica pain for more than 10 years. +1.2 BILLION € spent on sciatica related expenses. +8 MILLION people are affected by sciatica in Morocco only. +30% PATIENTS suffer from sciatica in the world. Daily Challenges:\nHigh prevalence of sciatica affecting a large population. Economic impact costing billions of euros annually in healthcare. Challenges in managing activities. Lack of personalized care. Mobility limitations. Solution Connected insoles that track the user’s daily activities using multiple sensors for the main purpose of pain prediction. Also providing instant feedback and correction recommandations.\nReal-time tracking of daily, weekly, and monthly activities. Predictive pain analysis. Personalized Recommendations. Daily, weekly and monthly reports. This is just the beginning, and I look forward to more opportunities to create meaningful change and to collaborate with remarkable individuals dedicated to shaping a better tomorrow. 🌍\nTogether, we are making a real difference! 🚀\nMore than just an AI Project Throughout this internship, I\u0026rsquo;ve had the privilege of collaborating with a diverse range of individuals - from fellow interns and dedicated supervisors to mentors who have guided me every step of the way. This experience also opened doors for me to interact with top-level directors and even the government minister of Economic Inclusion, Small Business, Employment and Skills Mr. Sekkouri Youness, allowing me to pitch in front of a diverse and influential audience, and pushing me to grow and expand my horizons in unexpected ways.\nBut it\u0026rsquo;s not just about the positions and titles; it\u0026rsquo;s about the people and their talents. Working alongside these gifted individuals, I\u0026rsquo;ve not only learned and honed my skills but also had the pleasure of collaborating with some of the brightest minds in the industry.\nHowever, the beauty of this internship extends beyond the workplace. It\u0026rsquo;s also about forging genuine connections and building meaningful relationships. Beyond projects and tasks, I\u0026rsquo;ve been fortunate to connect with amazing individuals and create lasting friendships. It\u0026rsquo;s a testament to the fact that professional growth is intrinsically tied to the people you meet and the experiences you share.\nI\u0026rsquo;m excited about the future and the potential opportunities that lie ahead, all thanks to the foundation I\u0026rsquo;ve built during my time at Orange. I look forward to staying connected and continuing to grow both professionally and personally.\nHere\u0026rsquo;s to the wonderful journey of growth, learning, and friendship! 🙌\nFor curious Minds If you\u0026rsquo;re interested in the project, visit it\u0026rsquo;s website and/or go to \u0026lsquo;Projects\u0026rsquo; in this portfolio. I have detailed the technical aspects of the project more there.\n","permalink":"https://abirarsalane.github.io/blog/osc/","summary":"Innovation Meets Impact: My Journey with Orange Summer Challenge 2023 Can you imagine a day where innovation meets social impact? I had the privilege of living it on October 17th, 2023, as we celebrated the culmination of the Orange Summer Challenge 2023. This prestigious program brought together brilliant minds from 15 countries across the Orange Digital Centers network, uniting our collective vision to shape the future. 🌍\nThis year, the Orange Summer Challenge revolved around the theme \u0026ldquo;Artificial Intelligence and Internet of Things: A Winning Duo to Tackle the Challenges of Tomorrow!","title":"I solved a problem that 600+ MILLION people suffer from using AI "},{"content":"Introduction Have you ever thought of knowing what moroccan people in Morocco on social media think about Climate Change? And no, not in english, not in french, and definitely not using latin alphabet (Fun Fact: Darija can be typed in latin alphabet in case you didn\u0026rsquo;t know, and it would have made the job way easier). If the answer is yes, it\u0026rsquo;s your lucky day, because I\u0026rsquo;ve already done all the heavy lifting and now you can enjoy the results. The idea behind this project is scrapping ALL social media platforms to understand what moroccans think about climate change.\nResults and process Read this first, it combines everything. If you have more time, you can scroll down for more details.\nChallenges Scapping Scrapping was A CHALLENGE, not only because scrapping from social media platforms is already a challenge (it\u0026rsquo;s no secret), but also because NO ONE talked about climate change (at least not in darija), you can find moroccans talking about the issue in english, french or darija in latin alphabet never in arabic alphabet, so scrapping took a LONG LONG time. This goes without even mentionning the struggle of scrapping from facebook, instagram, youtube, twitter (X)..\nPre-processing Preprocessing data in darija? Yes, no problem, it\u0026rsquo;s not like we combine 2,3,4 words in one..\nLabeling Ok, good luck trying to understand what the person ACTUALLY meant with what they said. Are they happy? sad? Was that sarcastic? It\u0026rsquo;s time to put Machine Learning aside and tackle psychology because you know, Machine Learning is definitely NOT the challenge here. And even if you don\u0026rsquo;t have to worry about the emotion, you will definitely have to worry about the intentions and context, because the same sentence can have many many meaning depending on the context and the intentions of the person. Picture this: I was literally reading teh sentence out loud and trying to match the vibe through different tonations. FUN TIME!\nModels Training After all the hard work done before, we have finally reached the Machine Learning part, and surprisingly (or not) it\u0026rsquo;s the easiest. All I had to do was improve the accuracy of the best performing model. In this case it was Logistic Regression. After that, I implemented K-means to do Topic Modeling finding out that there are 3 main topics: political, religious and social.\nData Viz And finally, I plotted the variation of emotions throught the months and the rate of comments in the matter globally, and for each topic.\n","permalink":"https://abirarsalane.github.io/projects/sentimentanalysis/","summary":"Introduction Have you ever thought of knowing what moroccan people in Morocco on social media think about Climate Change? And no, not in english, not in french, and definitely not using latin alphabet (Fun Fact: Darija can be typed in latin alphabet in case you didn\u0026rsquo;t know, and it would have made the job way easier). If the answer is yes, it\u0026rsquo;s your lucky day, because I\u0026rsquo;ve already done all the heavy lifting and now you can enjoy the results.","title":"Sentiment Analysis on Climate Change in Morocco"},{"content":"Overview Introduction Ductal Carcinoma in Situ (DCIS) is a non-invasive, early stage of breast cancer where abnormal cells are found in the milk ducts of the breast. Early detection and treatment are crucial for preventing it from becoming invasive. Utilizing advanced AI technologies, this project aims to revolutionize the detection and diagnosis process, ensuring timely and accurate medical interventions.\nObjectives Optimization of Medical Resources: AI-driven automation for efficient resource utilization. Tailored Treatment Recommendations: AI models provide personalized treatment plans. Monitoring Disease Progression: Continuous AI monitoring for disease tracking. Patient-Centered Care: Enhancing patient experience through AI-driven insights. Benchmarking and Validation: Ensuring high standards through AI-based evaluations. Personalized Diagnosis: Leveraging AI for individualized diagnosis. Early Detection: AI algorithms identify DCIS at its earliest stages. Treatment Response: Monitoring and improving responses to treatments through AI analytics. Project Context Importance of Early Detection Early detection allows for prompt intervention, potentially preventing the progression of DCIS to invasive breast cancer. Accurate detection reduces unnecessary aggressive treatments, promoting a targeted and less invasive approach. AI plays a pivotal role in enhancing detection accuracy and efficiency, which is critical for improving patient outcomes.\nGoals Improving Survival Rates: Early detection improves overall survival rates. Reducing Treatment Intensity: Accurate detection reduces the need for aggressive treatments. Empowering Patients: Providing early information enables informed decision-making. Project Approach Project Process Establish Objective Develop MVP (Minimum Viable Product) State-of-the-Art (SOTA) \u0026amp; Benchmarking Data Collection \u0026amp; Preprocessing Iterative Testing and Validation Optimization Continuous Refinement Deployment Design Thinking Empathize Phase\nPatient Data Analysis: Examination of patient records and histories using AI analytics. Interviews with Patients: Understanding their experiences and needs. Case Studies Review: Analyzing detailed case studies. Insights Gained Emotional Impact: High levels of anxiety and stress among patients. Need for Clarity: Struggle to understand medical information. Desire for Swift Diagnosis: Need for quicker diagnostic processes. Challenges in Early Detection For Patients: Anxiety and uncertainty. For Healthcare Providers: Resource limitations and information gaps. How AI Can Help Reducing Diagnostic Time: AI tools rapidly analyze medical images and data. Enhancing Diagnostic Accuracy: AI algorithms, trained on extensive datasets, improve identification of CCIS. Improving Patient Communication: AI-driven platforms provide clear, understandable information. Pre-prototype State-of-the-Art Technologies Transformers: Vision Transformer (ViT): This model applies transformer architectures to image patches, treating them similarly to word tokens in natural language processing. It has shown remarkable performance in image classification tasks due to its ability to capture long-range dependencies. Swin Transformer: This model introduces the concept of shifted windows, which allows for the efficient computation of both local and global self-attention. This makes it highly effective for high-resolution medical imaging tasks. Convolutional Vision Transformer (CvT): Combining the strengths of CNNs and transformers, CvT utilizes convolutional layers to create hierarchies of feature representations, enhancing the model\u0026rsquo;s ability to handle image data efficiently. Convolutional Neural Networks (CNNs): ResNet-50 and ResNet-101: These deep residual networks are known for their ability to train very deep networks by using skip connections, which help in mitigating the vanishing gradient problem. EfficientNet: A family of models that scale depth, width, and resolution in a structured manner, leading to better performance with fewer computational resources. U-Net and its variants: Widely used in medical image segmentation, U-Net architectures are designed to work well with limited data and provide precise localization. Generative Models: Generative Adversarial Networks (GANs): Used for generating high-quality synthetic medical images, GANs can augment training datasets, thus enhancing the robustness of the model. Variational Autoencoders (VAEs): Employed for anomaly detection and image reconstruction, VAEs help in learning efficient latent space representations of the input data. Libraries and Tools Scraping and Data Preprocessing Scrapy: An open-source and collaborative web crawling framework for Python, used to extract the data from websites. BeautifulSoup: A library for parsing HTML and XML documents and extracting data. Pandas: A powerful data manipulation and analysis library for Python, providing data structures like DataFrames. NumPy: A library for numerical computing in Python, providing support for arrays and matrices, along with a collection of mathematical functions. Deep Learning Frameworks TensorFlow: An open-source platform for machine learning, TensorFlow is widely used for building and deploying machine learning models due to its comprehensive ecosystem. PyTorch: Known for its dynamic computation graph and ease of use, PyTorch is favored for research and development in deep learning. Keras: A high-level API for building and training deep learning models, Keras simplifies the model creation process. Specialized Tools for Medical Imaging MONAI: Medical Open Network for AI (MONAI) is specifically designed for developing medical imaging models, providing specialized data loaders, transformations, and network architectures. SimpleITK: A simplified layer built on ITK (Insight Segmentation and Registration Toolkit), it is used for medical image processing and analysis. Data Augmentation Libraries Albumentations: This library provides a fast and flexible set of tools for augmenting images, which is crucial for training robust deep learning models. Metrics Key Performance Metrics Accuracy: Measures the overall correctness of the model’s predictions. Recall (Sensitivity): The ability of the model to correctly identify all positive cases of CCIS, crucial for minimizing false negatives. Precision: The correctness of positive predictions, ensuring that identified cases are indeed positive. F1 Score: The harmonic mean of precision and recall, providing a balance between the two metrics. Dice Coefficient: Used primarily in image segmentation tasks, this metric measures the overlap between predicted and actual segmentation masks, critical for evaluating the performance of medical image analysis models. Data Governance Implementation Steps Data Acquisition: Ensuring proper patient consent and accurate collection of high-quality medical data. Data Storage and Access: Implementing secure storage solutions with strict access controls to protect patient data. Data Processing: Ensuring data is processed in a confidential and compliant manner, leveraging techniques like data anonymization and encryption. Compliance Checks: Regularly conducting audits to ensure adherence to data privacy laws and regulations such as GDPR and HIPAA. Innovation and Adaptability: Continuously refining data governance practices to incorporate new technologies and respond to emerging threats. Impact of Effective Data Governance Reliable AI Analysis: Establishing a solid foundation for accurate and trustworthy diagnostics. Compliance and Trust: Building patient and stakeholder trust through ethical and legal data practices. Personalized Healthcare: Facilitating the development of customized treatment plans based on secure and accurate data. Risk Mitigation: Reducing errors and enhancing patient safety through robust data handling practices. Driving Innovation: Encouraging the adoption of cutting-edge AI solutions while maintaining high standards of data security and integrity. Conclusion This project leverages cutting-edge AI technologies to significantly enhance the early detection and diagnosis of DCIS. By focusing on patient needs, reducing diagnostic times, and improving the accuracy of diagnoses, this initiative aims to transform breast cancer care. Effective data governance and the innovative use of state-of-the-art AI models underpin the project\u0026rsquo;s success, promising substantial advancements in healthcare delivery and patient outcomes.\n","permalink":"https://abirarsalane.github.io/blog/dcis-project/","summary":"Overview Introduction Ductal Carcinoma in Situ (DCIS) is a non-invasive, early stage of breast cancer where abnormal cells are found in the milk ducts of the breast. Early detection and treatment are crucial for preventing it from becoming invasive. Utilizing advanced AI technologies, this project aims to revolutionize the detection and diagnosis process, ensuring timely and accurate medical interventions.\nObjectives Optimization of Medical Resources: AI-driven automation for efficient resource utilization. Tailored Treatment Recommendations: AI models provide personalized treatment plans.","title":"AI-Powered Early Detection and Diagnosis of Ductal Carcinoma In Situ (DCIS)"},{"content":"Sorbonne University What a journey has it been! Last semester was one of the most challenging yet exciting semesters I’ve ever had. It was full of ups and downs, adventures, challenges, opportunities but more importantly knowledge.\nI’ve learnt so much and grown so much and I can’t even begin to explain how enriching my experience at Sorbonne Université has been. I met amazing people, worked with brilliant students and learned from the best professors.\nI’m so proud to have been a student at one of the most prestigious universities in France and the world.\nI am beyond proud and honoured to have had the chance to be enrolled in the program Licence d’informatique at Sorbonne Université that is ranked #1 in the “licence d’informatique” programs ( bachelors of computer science) in France (according to the Classement Eduniversal des meilleurs Licences, Bachelors et Grandes Écoles).\nThis experience was enriching on so many levels. I had the chance to take very intersting classes such as \u0026lsquo;Bioinformatics\u0026rsquo; where I learnt a lot about biology and how to implement computer science and specifically data science to solve biology challenges. I have also had the opportunity to be part of many interesting events such as the 4EU+ Alliance event where I had the pleasure to represente Sorbonne University among 6 other european universities (Because you know Youth has a voice and I\u0026rsquo;ll gladly be it!). I was also a referee at the France Cup of Robotis which was a ONE-OF-A-KIND experience. And other than professional experiences, I was also part of many cultural events. One of them was the \u0026lsquo;Emily in Paris\u0026rsquo; tour when I, with otehr international students, explored the places where the series were filmed.\nAll in all, it was an incredible experience and I\u0026rsquo;m so thankful to have had these opportunities.\nI’m so thrilled and excited for what’s coming next!\n","permalink":"https://abirarsalane.github.io/blog/sorbonne-univ/","summary":"Sorbonne University What a journey has it been! Last semester was one of the most challenging yet exciting semesters I’ve ever had. It was full of ups and downs, adventures, challenges, opportunities but more importantly knowledge.\nI’ve learnt so much and grown so much and I can’t even begin to explain how enriching my experience at Sorbonne Université has been. I met amazing people, worked with brilliant students and learned from the best professors.","title":"What it's like to study in the best CS bachelors program in the world"},{"content":"6-weeks of creativity and hard work I\u0026rsquo;m happy to announce that I\u0026rsquo;m starting a new journey of growth, learning and challenges. For the next 6 weeks, I, and many others, will be working hard to create projects we\u0026rsquo;re passionate about. I\u0026rsquo;ll write updates on this page as we move forward in this journey!\n","permalink":"https://abirarsalane.github.io/blog/n-and-w/","summary":"6-weeks of creativity and hard work I\u0026rsquo;m happy to announce that I\u0026rsquo;m starting a new journey of growth, learning and challenges. For the next 6 weeks, I, and many others, will be working hard to create projects we\u0026rsquo;re passionate about. I\u0026rsquo;ll write updates on this page as we move forward in this journey!","title":"Nights And Weekends program by BuildSpace"},{"content":"Description Data Scientist in the Innovation Stream of \u0026lsquo;Data and AI\u0026rsquo; at Orange Business\nDuring my time at Orange Business, I had the opportunity to work on two significant projects: \u0026ldquo;Orange Business Forecasting and Transformation: Dynamic Forecasting\u0026rdquo; and \u0026ldquo;Cybersecurity Operations with AI-Driven Threat Hunting.\u0026rdquo; These projects allowed me to leverage my Data Science, Machine Learning and Generative Artificial Intelligence skills to drive innovation and improve operational efficiency.\nProject 1: Orange Business Forecasting and Transformation: Dynamic Forecasting Objective: Study the transformation of Orange Business clients, forecast future needs, discover patterns, and adapt strategies accordingly.\nExperience Overview As a Data Scientist in the Innovation Stream at Orange Business, I was integral to the \u0026ldquo;Orange Business Forecasting and Transformation: Dynamic Forecasting\u0026rdquo; project. My role focused on studying the transformation of Orange Business clients, forecasting future needs, discovering patterns, and adapting strategies accordingly. This project allowed me to leverage my technical and analytical skills to drive innovation, optimize operations, and deliver strategic insights, significantly contributing to the organization\u0026rsquo;s data and AI capabilities.\nKey Responsibilities and Achievements: Data Collection and Pre-processing:\nData Gathering:\nCollaborated with various departments, including financial and marketing teams, to gather comprehensive datasets. Extracted data from PostgreSQL using pgAdmin, performing complex queries to retrieve and integrate data from multiple sources. Data Pre-processing:\nConducted extensive data pre-processing to handle various data quality issues such as formatting inconsistencies, null values, and missing values. Standardized different namings for the same products or clients across departments using a mapping dataset, ensuring data consistency and accuracy. Employed SQL for data retrieval from databases, managing both batch and real-time data ingestion to ensure data completeness and relevance. Data Engineering and Pipeline Development:\nPipeline Development:\nBuilt and maintained data pipelines using tools like GitLab, Jupyter, and Python, ensuring efficient data processing and transformation. Created robust ETL processes to handle data ingestion, transformation, and loading, ensuring data consistency and integrity. Data Manipulation:\nUtilized Pandas for efficient data manipulation, particularly for time series forecasting, with features like Series objects, date-time indexes, and data transformations such as offset, delay, and padding. Leveraged Scipy\u0026rsquo;s squareform function to convert shape-vector distance matrices into square distance matrices for better data visualization and manipulation. Machine Learning and Model Development:\nModel Implementation:\nDeveloped models using Python libraries such as Scipy, Sklearn, and Pandas for data manipulation, similarity calculations, and time series forecasting. Implemented various forecasting models using R libraries, including: Forecast Library: Used for modeling and forecasting time series with algorithms like ETS, ThetaF, and NNetAR. Metrics Library: Applied for evaluating model performance using error measures such as RMSE, MAE, and MSE. Tidyverse Library: Employed for data manipulation, including dplyr for data manipulation and ggplot2 for data visualization. Purrr Library: Utilized for functional programming in R, manipulating functions and vectors. PMCMRplus Library: Conducted post-hoc multiple comparison tests like the Friedman test to identify significant differences between models. Model Evaluation:\nConducted a detailed comparison of RMSE distributions for different models, applying statistical tests like the Friedman test to rank models. Implemented revenue prediction using various models: Naive Model: Noted for its greater variability in prediction errors. ETS and Theta Models: Exhibited less dispersed RMSE distributions, indicating greater accuracy. NNetAR Model: Showed some points with significantly higher RMSE, suggesting the presence of outlier errors. Visualization and Reporting:\nDashboard Creation:\nDesigned and developed interactive dashboards using Power BI to visualize key metrics and trends, including: Customer Comparison Dashboard: Provided an overall view of developments by client. Revenue Evolution Dashboards: Showcased the temporal evolution of revenue per customer, by category, and by product. Customer Typology Dashboards: Created typologies using Jaccard distances and visualized these with squareforms, dendrograms, and point clouds representing distances between groups. Report Generation:\nProduced comprehensive reports summarizing clustering characteristics and forecasting results, aiding strategic decision-making. Included bar charts grouped by customer, treemaps of DGC_categories, relative gain tables, and absolute value difference tables for in-depth revenue analysis. Operational Efficiency and Collaboration:\nAutomation and Testing:\nAutomated end-to-end testing using PyTest, significantly reducing manual testing time and improving development efficiency. Built CI pipelines with GitHub Actions to streamline the building and testing of container images, ensuring consistent and reliable deployments. Collaboration Tools:\nUtilized Microsoft Teams and Confluence for effective team communication and documentation, facilitating seamless collaboration across departments. Strategic Infrastructure Optimization:\nDeployment and Scalability: Deployed solutions in a Dev/UAT environment, with plans to create an automated application in the Enterprise Data Hub (EDH) for real-time data injection and live interaction. The EDH platform, based on HortonWorks products, offers big data services, security, and tools for data ingestion, enrichment, and management. Technical Tools and Technologies Used: Programming Languages: Python (for data manipulation, modeling, and automation) R (for advanced statistical analysis and forecasting) Libraries and Frameworks: Scipy, Sklearn, Pandas (for data processing and machine learning in Python) Forecast, Metrics, Tidyverse, Purrr, PMCMRplus (for time series forecasting and data analysis in R) Data Visualization: Power BI (for creating interactive dashboards) Version Control and Collaboration: GitLab (for version control) Microsoft Teams, Confluence (for team collaboration and documentation) Databases: PostgreSQL, pgAdmin (for data extraction and management) Cloud and Infrastructure: Enterprise Data Hub (EDH) (for big data services and security) Impact and Learning Outcomes: Enhanced Forecasting Accuracy: Improved forecasting models, leading to more accurate predictions of client needs and revenue trends. Operational Efficiency: Achieved significant reductions in manual testing time and cloud infrastructure costs through automation and optimization. Strategic Insights: Provided actionable insights through comprehensive dashboards and reports, supporting data-driven decision-making. Collaboration and Communication: Fostered effective collaboration across departments, enhancing overall project outcomes and stakeholder satisfaction. Project 2: Cybersecurity Operations with AI-Driven Threat Hunting Objective: Develop an AI-driven solution for automated threat hunting and response in cybersecurity, leveraging large language models (LLMs).\nExperience Overview As a Data Scientist in the Innovation Stream at Orange Business, I was integral to the \u0026ldquo;Cybersecurity Operations with AI-Driven Threat Hunting\u0026rdquo; project. By leveraging advanced AI techniques, RAG with private data, and cloud technologies, I successfully developed and implemented a solution that significantly enhanced threat detection and response capabilities, improved operational efficiency, and strengthened the overall cybersecurity posture of the organization.\nKey Responsibilities and Achievements: Solution Development and Implementation:\nAutomated Playbooks:\nUtilized GPT-4 APIs and Azure OpenAI to develop automated playbooks for threat hunting and response. Implemented Retrieval-Augmented Generation (RAG) with private data (e.g., logs) to generate tailored playbooks for various attack scenarios including malware propagation, data exfiltration, and insider threats. Designed playbooks that include comprehensive steps for threat detection and mitigation, integrating Indicators of Compromise (IOCs), Tactics, Techniques, and Procedures (TTPs), queries, and response actions. Threat Identification and Prioritization:\nDeveloped machine learning models to analyze and identify threats from log data and network traffic. Prioritized threats based on severity and potential impact, using AI to enhance decision-making processes. Incorporated insights from technological trends and market developments to refine threat models and improve detection accuracy. Proactive Threat Hunting:\nLeveraged AI to proactively hunt for early signs of compromise by analyzing patterns and anomalies in security data. Provided security teams with real-time intelligence to preemptively address potential threats. Operational Efficiency and Resource Optimization:\nResource Optimization:\nAutomated routine security tasks such as log analysis and threat classification, using machine learning to reduce manual workload. Ensured rapid updates to detection and response guidance by continuously training models with new threat data. Enhanced Cybersecurity Posture:\nDeployed AI-driven solutions to continuously monitor and respond to security threats, enhancing the organization’s overall cybersecurity posture. Improved the efficiency of security operations by automating detection and response processes, allowing teams to focus on complex threats. Technical Implementation:\nTechnology Stack:\nDeveloped the solution using Python and its libraries, including Scikit-learn for machine learning, Pandas for data manipulation, and TensorFlow for deep learning models. Utilized GPT-4 APIs for natural language processing tasks and Azure OpenAI for model deployment and scalability. Implemented RAG techniques to integrate private data (logs) with AI models, improving the relevance and accuracy of generated playbooks. Development and Deployment:\nDesigned a multi-stage AI pipeline to process and analyze security data, including data preprocessing, feature extraction, model training, and deployment. Deployed the solution using Kubernetes for container orchestration, ensuring scalability and high availability. Packaged the solution as a Helm chart for easy deployment, distribution, and versioning. Monitoring and Optimization:\nEstablished robust monitoring and logging systems using Grafana Agent and Grafana Cloud to track the performance and effectiveness of AI models. Employed continuous integration/continuous deployment (CI/CD) pipelines with GitHub Actions to automate the building and testing of container images. Continuously optimized AI algorithms to improve threat detection accuracy and reduce false positives. Impact and Learning Outcomes:\nImproved Threat Detection:\nAchieved faster and more accurate threat detection, significantly reducing the mean time to identify and respond to security incidents. Enhanced the capability to detect and mitigate sophisticated attack scenarios, providing better protection for the organization. Increased Efficiency:\nStreamlined security operations by automating routine tasks, freeing up security analysts to focus on strategic initiatives and complex threat analysis. Enabled security teams to prioritize and address high-priority threats more effectively, improving overall operational efficiency. Scalable and Robust Solution:\nDelivered a scalable and robust AI-driven cybersecurity solution that can adapt to evolving threats and organizational requirements. Ensured the solution’s resilience and reliability through continuous monitoring, logging, and optimization. Summary Through these projects, I developed a comprehensive skill set in data science, machine learning, cloud computing, and cybersecurity. My ability to apply these skills to real-world challenges demonstrates my technical mastery and commitment to delivering high-impact solutions.\n","permalink":"https://abirarsalane.github.io/experience/ob/","summary":"Description Data Scientist in the Innovation Stream of \u0026lsquo;Data and AI\u0026rsquo; at Orange Business\nDuring my time at Orange Business, I had the opportunity to work on two significant projects: \u0026ldquo;Orange Business Forecasting and Transformation: Dynamic Forecasting\u0026rdquo; and \u0026ldquo;Cybersecurity Operations with AI-Driven Threat Hunting.\u0026rdquo; These projects allowed me to leverage my Data Science, Machine Learning and Generative Artificial Intelligence skills to drive innovation and improve operational efficiency.\nProject 1: Orange Business Forecasting and Transformation: Dynamic Forecasting Objective: Study the transformation of Orange Business clients, forecast future needs, discover patterns, and adapt strategies accordingly.","title":"Data Scientist Intern"},{"content":"Description I have participated in many events, competitions, hackathons, etc. Here are some of the most important ones.\nAlumnus of Mathematics Olympiads Been a mathematics olympiads participant for 5 years, from middle school to high school, while being a national winner in middle school. Attended mentorship sessions and trainings from previous international finalists and Ecole Polytechnique (X) students. Alumnus of Informatics Olympiads Been an informatics olympiads participant for 3 years, while being ranked 1st in my school and 9th in the country in the Castor contest. International Youth Math Challenge participant Spelling Bee Regional Winner Mental arithmetic National Finalist Nasa Space Apps Challenge Covid Edition Finalist The project Rosette has been selected among the 40 world finalists in the hackathon from over 150.00 participants worldwide. Rosette is a mobile application that demonstrates the changes in the local natural environment in Africa due to covid-19. Nasa Space Apps Challenge 2020 Honorable Mention The projet AlphaUmi got an honorable mention on the NASA space apps challenge hackathon of 2020. AlphaUmi is an interactive game that allows users to relive past missions of NASA. The application can be found on my GitHub account. Act In Space Winner Won first place nationally Been selected as finalist for the international competition Won 3rd place internationally- Audience Award The project SkyWatch is a mobile application that teaches kids about space and astronomy. Competitive Programmer in : GirlsCode, JNJD, ACPC, CodEMI,.. Participated regularly in various Competitive Programming competitions with different rankings and performances. ","permalink":"https://abirarsalane.github.io/achievements/distinction/","summary":"Description I have participated in many events, competitions, hackathons, etc. Here are some of the most important ones.\nAlumnus of Mathematics Olympiads Been a mathematics olympiads participant for 5 years, from middle school to high school, while being a national winner in middle school. Attended mentorship sessions and trainings from previous international finalists and Ecole Polytechnique (X) students. Alumnus of Informatics Olympiads Been an informatics olympiads participant for 3 years, while being ranked 1st in my school and 9th in the country in the Castor contest.","title":"Distinctions"},{"content":"Description As part of: Orange Summer Challenge (In collaboation with AWS and EY).\nOrange Summer Challenge is a prestigious, highly selective (1.7% acceptance rate) program that consists of a 4-months internship and competition. When interns are accepted in the program, they\u0026rsquo;re divided into teams that compete against each other in this challenge (and internship) for the next 4 months. The goal is to work on innovative projects in the edition\u0026rsquo;s theme. During my participation, the theme was AiOT: Artificial Intelligence and Internet of Things: A Winning Duo to Tackle the Challenges of Tomorrow! From Brainstorming to pitching the final product on stage, the journey is nothing short of exciting. Hard work, discipline, team work, collaboration, innovation, creativity, coding, problem-solving, critical thinking, public speaking and more are essential skills to win this challenge. This program is in collaboration with AWS and EY who provide mentorship and support throughout the entire program.\nKey Achievements and Responsibilities Selected from 23 interns out of 1300 candidates:\nOne of 23 interns, and one of 3 AI and data science selected profils from 1300 applicants through rigorous hiring process. Demonstrated proficiency in Python, R, SQL, ML, DL, algorithms, cloud computing, leadership, problem-solving and teamwork. 3rd place Orange Summer Challenge Prize:\nSecured 3rd place in the competition. Pitched our project infront of hundreds of people including ministers, directors, chiefs from AWS, EY and Orange, investors, professors amoung many. Achieved a high accuracy rate demonstrating my ability to apply theoretical knowledge to practical applications, solve complex problems, and deliver impactful results. Led a team of 6 as Scrum Master, boosting efficiency by 20%:\nAs a Scrum Master, I led a cross-functional team of six increasing efficiency by 20% through agile sprints. Utilized Jira for project management. Conducted daily stand-ups, sprint planning sessions, and retrospectives and facilitated open communication and continuous improvement within the team. Mentored team members on agile best practices, fostering a collaborative and high-performing team environment. Managed AWS account for a 30% latency reduction:\nServed as Cloud Team Leader to manage our team\u0026rsquo;s AWS account, optimizing our cloud infrastructure by implementing autoscaling groups, optimizing instance types, and leveraging Elastic Load Balancing to reduce latency by 30% and improve performance. Utilized AWS services such as EC2 for scalable computing, RDS for managed relational databases, and CloudFront for content delivery. Utilized AWS SageMaker for model training and deployment. Employed AWS CloudWatch for monitoring and logging, enabling proactive identification and resolution of performance issues. These optimizations significantly enhanced the responsiveness and efficiency of our applications, leading to a better user experience. Built improved Machine Learning models, achieving 15% accuracy increase:\nImproved model accuracy by 15% through hyperparameter tuning using GridSearchCV and RandomizedSearchCV, and implemented feature selection techniques to enhance model performance and feature selection. Used Python libraries (Scikit-learn, XGBoost, TensorFlow) and ensemble methods (bagging, boosting). My work involved rigorous cross-validation to ensure the robustness and generalizability of the models. Utilized Python for data preprocessing and feature engineering, and AWS SageMaker for model training and deployment. Demonstrated my deep understanding of machine learning techniques and my ability to drive significant improvements through systematic experimentation and optimization through iterations and continuous improvements of the models. Designed and managed data pipelines, orchestrating data flow from diverse sources through batch and streaming workflows:\nDesigned and managed robust data pipelines to handle data from diverse sources, ensuring seamless data flow and integration. Using Apache Kafka for real-time data streaming and Apache Airflow for batch processing, I orchestrated complex workflows that handled data ingestion, transformation, and loading. Employed ETL tools such as Talend and AWS Glue for data transformation and utilized Amazon S3 for scalable storage solutions. By implementing monitoring and logging solutions with ELK Stack (Elasticsearch, Logstash, Kibana), I ensured real-time visibility into data pipeline performance. This enabled timely troubleshooting and optimization, supporting real-time analytics and data-driven decision-making processes. Participated in meetings with Orange, EY and AWS managers and mentors\nParticipated in weekly meetings with Orange managers to discuss the project\u0026rsquo;s advancements, needs, and future steps. Conducted regular meetings with AWS and EY experts for updates, mentorship and support. Conducted in-dept research:\nDeveloped a dataset for predicting movement, activity, and posture using sensor-equipped insoles. Collaborated with a medical doctor to ensure data relevance and comprehensiveness. Refined models through feature engineering and machine learning techniques. Pitched the project and advertised it\nParticipated in many shootings for the project such as the demo video and the teaser video. Wrote the script of the videos, and did the voice-over. Participated in many shootings for the program\u0026rsquo;s social media platforms including the \u0026ldquo;Can you guess our solution?\u0026rdquo; video. Pitched our solution infront of ministers, directors, chiefs from AWS, EY and Orange, investors, professors amoung many, on stage and in the stand. Through these experiences, I have developed a comprehensive skill set in cloud computing, data science, machine learning, and agile methodologies. My ability to apply these skills to real-world challenges demonstrates my technical mastery and my commitment to delivering high-impact solutions.\n","permalink":"https://abirarsalane.github.io/experience/odc/","summary":"Description As part of: Orange Summer Challenge (In collaboation with AWS and EY).\nOrange Summer Challenge is a prestigious, highly selective (1.7% acceptance rate) program that consists of a 4-months internship and competition. When interns are accepted in the program, they\u0026rsquo;re divided into teams that compete against each other in this challenge (and internship) for the next 4 months. The goal is to work on innovative projects in the edition\u0026rsquo;s theme.","title":"Cloud Data Scientist and AI Engineer Intern"},{"content":"Description I like to help when I can. I volunteer a lot. Here are just a few of my volunteering experiences that are dear to me the most.\nEMPIMO member and organizer EMPIMO, or the Moroccan IMO Preparation Team, is a large virtual community of students gifted in competitive mathematics whose goal is to spread the culture of IMOs, provide the necessary orientation and resources to participants, maintain the fluidity of communication Olympic news between supervisors and students as well as providing a space for the exchange of questions and supporting any initiative in this context such as the formation of school clubs and the organization of mathematical competitions, forums and conferences. EMPIMO is the first exchange platform used by Olympic managers and candidates. Since 2017, EMPIMO has managed to carry out several educational projects, including: 2 editions of the Olympic Mathematics Forum (FMO), 2 editions of the Junior Combinatory Olympiad (OCJ), as well as this large digital media library. WomenTech member WomenTech Network strives to be the leading global organization for women in tech and allies by providing opportunities for professional growth, networking, and career advancement. I volunteered as a WomenTech Student Ambassador to promote and grow the community of Women In Tech. I volunteered as a WomenTech Outreach Lead for the Global Conference of WomenTech AIESEC volunteer AIESEC is a youth-run, international non-governmental and non-profit organization that helps young people develop leadership skills practical experiences of many kinds, including internships, volunteering opportunities and fosters cross-cultural ties through its global volunteer exchange program. I volunteered as a OGV (Ongoing Global Volunteer) Team Leader, helping young people from my country volunteer abroad. I volunteered as a Community Managment Team leader, coordinating between community management, design, writing teams, and managing social media platforms. I volunteered as a Talent Manager at AIESEC. Enactus volunteer Enactus is a student organization that develops student entrepreneurship within the institutional environment. Enactus is the largest experiential learning platform that develops NextGen Leaders with a head for business and a heart for the world. I volunteered a vice-president team lead to coordinate between student entrepreneurs with projects and ENACTUS and also help them through workshops, events,.. Referee Volunteer for Robotics Competitions I volunteered as a referee for the Robotics France Cup. I volunteered as a referee for the E-Robotics for european countries. I volunteered as a referee for the Robotics France Cup Junior. I volunteered as an animator for VEX Robotics. Ensured fair play and adherence to competition rules in 20+ matches. ","permalink":"https://abirarsalane.github.io/achievements/volunterring/","summary":"Description I like to help when I can. I volunteer a lot. Here are just a few of my volunteering experiences that are dear to me the most.\nEMPIMO member and organizer EMPIMO, or the Moroccan IMO Preparation Team, is a large virtual community of students gifted in competitive mathematics whose goal is to spread the culture of IMOs, provide the necessary orientation and resources to participants, maintain the fluidity of communication Olympic news between supervisors and students as well as providing a space for the exchange of questions and supporting any initiative in this context such as the formation of school clubs and the organization of mathematical competitions, forums and conferences.","title":"Volunteering"},{"content":"Description Having a life outside of classes and contributing to student\u0026rsquo;s life on campus is just as important as your academic performance and grades. Here are just a few of the most recent activities I\u0026rsquo;ve done. I have always been an active student, but I\u0026rsquo;ve only mentionned university extracurricular activities.\nFounder \u0026amp; president of Consulting Club Data4Development Collaborated with 2 startups to get 80+ students involved in solving real life use cases using data science. Co-Founder of the AI club Built a community of 130+ members and organized monthly workshops and seminars, achieving an average attendance rate of 80%. President of the Junior Committee of the IT Club Led the junior committee, mentoring and guiding 20 junior members in developing technical skills, resulting in a 40% increase in junior members actively participating in club activities. Communications manager of the university student council Managed 3 social media accounts improving engagement by 150%. Tech conferences and events organizer Organized 10+ tech conferences and events, with an average attendance of 200 participants per event. University mathematics and computer science tutor Increased passing rate by 50% for 70+ students. Vice President Team Lead at Enactus Oversaw 4 projects focused on social entrepreneurship, and organized weekly meetings and monthly events. Campus Director of Hult Prize Responsible of the organization of the Local Hult Prize competition and accompaniement of the participants. Representative of Sorbonne University at 4EU+ Alliance Represented Sorbonne University at the 4EU+ Alliance alongside with 6 other students each from a university from a european country event to represent youth and come up with ideas for collaborations, partnerships and events. ","permalink":"https://abirarsalane.github.io/achievements/extracurricular-activities/","summary":"Description Having a life outside of classes and contributing to student\u0026rsquo;s life on campus is just as important as your academic performance and grades. Here are just a few of the most recent activities I\u0026rsquo;ve done. I have always been an active student, but I\u0026rsquo;ve only mentionned university extracurricular activities.\nFounder \u0026amp; president of Consulting Club Data4Development Collaborated with 2 startups to get 80+ students involved in solving real life use cases using data science.","title":"Extracurricular Activities"},{"content":"Description Research Data Scientist at Electric Vehicle Insurance Startup\nI was interning as a research data scientist at a startup in Paris specializing in insurance for electric vehicles that was honored as the Coup de Coeur Climate Finance Fund at the Fintech For Tomorrow challenge and is part of the prestigious FrenchTech Grand Paris. My role focused on developing a scoring system to evaluate and incentivize eco-friendly driving behaviors. This score determined driver eligibility for awards based on their eco-responsibility.\nKey Responsibilities and Achievements Developing the Eco-Responsibility Score Score Formulation: Designed a comprehensive formula to calculate a driver’s eco-responsibility score, reflecting their energy consumption and driving efficiency. Conducted extensive research in physics, energy consumption, and factors affecting electric vehicle performance to ensure accuracy and relevance. Integrated variables such as speed, acceleration, braking patterns, route topology, and weather conditions into the scoring algorithm. Research and Data Analysis Interdisciplinary Research: Delved into various domains, including mathematics, climate science, and energy consumption. Read and analyzed numerous research papers to gather insights and data. Contacted dataset authors for additional information to supplement my models. Data Mining and Model Development: Sourced and curated datasets relevant to driving patterns and energy usage from sources like telematics data and public databases. Performed data cleaning and imputation to handle missing values and ensure data integrity. Enhanced modeling accuracy by 15% through advanced data mining techniques and feature engineering. Applied machine learning algorithms (e.g., Random Forest, Gradient Boosting) and statistical methods to derive meaningful insights from the data. Scraped the web for detailed consumption data of various electric vehicle models based on brand, series, and other specifications. Implementation and Impact Integration and Deployment: Successfully integrated the eco-responsibility score into the company’s services and products. Developed API endpoints and user interfaces to make the score accessible to end-users and integrate seamlessly with existing systems. The scoring system is actively used by end-users, contributing to the company’s offerings. Cross-disciplinary Collaboration: Worked closely with teams across different disciplines to ensure a holistic approach to the project. Collaborated with software engineers to ensure efficient and scalable implementation of the scoring algorithm. Boosted energy efficiency by 25% with innovative eco-driving modules, achieving a 20% accuracy improvement in driving behavior prediction. Continuous Improvement: Conducted iterative testing and refinement of the scoring algorithm to adapt to new data and improve performance. Utilized feedback from real-world usage to make ongoing enhancements to the model. Employed version control systems (e.g., Git) to manage code changes and maintain a robust development workflow. Technical Tools and Technologies Used Programming Languages: Python (for data analysis, modeling, and algorithm development) R (for statistical analysis) Data Handling and Processing: Pandas, NumPy (for data manipulation) Scikit-learn, TensorFlow (for machine learning) SQL (for database queries and management) Web Scraping: BeautifulSoup, Scrapy (for extracting vehicle consumption data from the web) Deployment: Docker (for containerization) Visualization and Reporting: Matplotlib, Seaborn (for data visualization) Jupyter Notebooks (for interactive analysis and reporting) Through this experience, I developed a deep understanding of the intersection between data science and sustainable practices in the context of electric vehicle insurance. My work not only advanced the company’s product offerings but also promoted eco-friendly driving behaviors, aligning with broader environmental goals.\n","permalink":"https://abirarsalane.github.io/experience/joltee/","summary":"Description Research Data Scientist at Electric Vehicle Insurance Startup\nI was interning as a research data scientist at a startup in Paris specializing in insurance for electric vehicles that was honored as the Coup de Coeur Climate Finance Fund at the Fintech For Tomorrow challenge and is part of the prestigious FrenchTech Grand Paris. My role focused on developing a scoring system to evaluate and incentivize eco-friendly driving behaviors. This score determined driver eligibility for awards based on their eco-responsibility.","title":"Research Data Scientist Intern"},{"content":"Description During my Geolocation internship at Integrytis, I undertook various responsibilities that significantly contributed to both my professional development and the success of the projects I worked on. My role involved a mix of documentation, analysis, development, and team collaboration. I used various technologies such as Dart/ Flutter, Git, Geolocations SDKs, and more.\nKey Responsibilities and Achievements Documentation of Fundamental Principles Comprehensive Documentation: Produced detailed documentation on the fundamental principles of the project, ensuring that complex concepts were clearly explained and easily accessible to both technical and non-technical stakeholders. The documentation covered theoretical foundations, architectural design, and practical implementation steps. Created user guides and API documentation to support developers and end-users in understanding and utilizing the software effectively. In-depth Analysis of Software and SDK Functional Analysis: Conducted an in-depth analysis of the software and its Software Development Kit (SDK), comparing functionalities with existing solutions in the market. Evaluated performance, usability, and feature sets to identify strengths, weaknesses, and areas for improvement. Provided detailed reports with recommendations based on the comparative analysis, aiding in strategic decision-making for product development. Development of Mobile Applications Dart/Flutter Development: Collaborated in the development of mobile applications using Dart and Flutter, contributing to both frontend and backend components. Implemented features that enhanced user experience, focusing on responsiveness, performance, and usability. Worked on UI/UX design elements to create intuitive and visually appealing interfaces. Integration of Geolocation SDK Geolocation Integration: Integrated the geolocation SDK into the mobile applications, enabling real-time location tracking and mapping functionalities. Ensured seamless integration with existing systems, maintaining high accuracy and reliability of location data. Tested the geolocation features extensively to ensure optimal performance under various conditions and scenarios. Team Coordination with Git Version Control and Collaboration: Used Git for version control, facilitating efficient team collaboration and code management. Managed branches, merges, and pull requests to ensure smooth workflow and integration of new features. Conducted code reviews to maintain code quality and adherence to best practices. Resolved conflicts and coordinated with team members to synchronize efforts and meet project deadlines. Technical Tools and Technologies Used Programming Languages Dart (for mobile app development with Flutter)\nFrameworks and Libraries Flutter (for cross-platform mobile application development)\nGeolocation Geolocation SDK (for real-time location tracking)\nVersion Control Git (for code management and team collaboration)\nDocumentation Markdown, Sphinx (for creating and maintaining comprehensive documentation)\nAnalysis and Reporting Excel, Jupyter Notebooks (for data analysis and reporting)\nImpact and Learning Outcomes Professional Growth Gained hands-on experience in mobile application development, geolocation integration, and team collaboration. Developed strong technical writing skills through the production of comprehensive documentation. Enhanced problem-solving abilities by analyzing software functionalities and providing actionable recommendations. Project Contributions Contributed to the successful development and deployment of mobile applications, improving the overall product offering. Ensured accurate and reliable geolocation functionalities, enhancing the application\u0026rsquo;s user experience. Supported team coordination and efficiency through effective use of Git and collaborative practices. This internship provided me with a well-rounded experience, combining technical development, analytical skills, and collaborative teamwork, all of which are crucial for a successful career in software engineering and development.\n","permalink":"https://abirarsalane.github.io/experience/integrytis/","summary":"Description During my Geolocation internship at Integrytis, I undertook various responsibilities that significantly contributed to both my professional development and the success of the projects I worked on. My role involved a mix of documentation, analysis, development, and team collaboration. I used various technologies such as Dart/ Flutter, Git, Geolocations SDKs, and more.\nKey Responsibilities and Achievements Documentation of Fundamental Principles Comprehensive Documentation: Produced detailed documentation on the fundamental principles of the project, ensuring that complex concepts were clearly explained and easily accessible to both technical and non-technical stakeholders.","title":"Geolocation Intern"},{"content":"Description Trainee at Google Get Ahead Program\nGoogle Get Ahead is an elite, invitation-only eight-week program tailored for exceptional female-identifying students eager to elevate their technical expertise and explore dynamic career opportunities at Google. The program offers a unique blend of immersive video training, challenging coding exercises, and invaluable access to Google’s top engineers.\nKey Responsibilities and Achievements: Intensive Video Trainings:\nAdvanced Technical Skill Development: Immersed myself in an extensive array of video trainings covering critical and advanced topics in computer science. Mastered complex concepts including algorithms, data structures, software engineering principles, and cloud computing. Applied theoretical knowledge to practical scenarios, reinforcing a deep understanding of the material. Rigorous Coding Challenges:\nSophisticated Problem-Solving and Algorithm Design: Tackled weekly coding challenges that tested and honed my problem-solving abilities and algorithmic proficiency. Utilized programming languages such as Python and Java to craft elegant, efficient solutions to intricate problems. Consistently ranked among the top performers, showcasing my ability to write optimized, high-performance code under pressure. Exclusive Access to Google Engineers:\nDirect Mentorship and Expert Guidance: Engaged in live Q\u0026amp;A sessions and one-on-one mentorship with seasoned Google engineers. Received detailed, personalized feedback on coding challenges and project work, significantly enhancing my technical skills. Gained insider insights into cutting-edge technologies and Google’s innovative engineering culture, directly from industry leaders. Collaborative Learning Environment:\nPeer Networking and Knowledge Exchange: Collaborated with a diverse group of talented peers through virtual study groups and interactive discussion forums. Shared insights, exchanged innovative ideas, and worked on collaborative coding projects, fostering a rich learning community. Built a robust network of like-minded individuals, creating opportunities for future collaboration and support. Career Exploration and Preparation:\nIn-depth Insight into Google’s Dynamic Work Environment: Explored a wide range of roles and career trajectories within Google, including software engineering, data science, and product management. Learned about Google\u0026rsquo;s recruitment process, including strategies for excelling in technical interviews and crafting standout resumes. Acquired a strategic understanding of the skills and experiences necessary to thrive in the tech industry’s most competitive environments. Technical Tools and Technologies Used:\nProgramming Languages: Python and Java (for advanced coding challenges and algorithm practice) Development Platforms: Google Cloud Platform (for hands-on cloud computing tasks and projects) GitHub (for sophisticated version control and collaborative coding) Communication and Collaboration: Google Meet (for live mentorship sessions and interactive Q\u0026amp;A) Slack (for seamless peer communication and collaborative discussions) Impact and Learning Outcomes: Remarkable Technical Proficiency: Significantly enhanced my coding and problem-solving skills through rigorous practice and expert mentorship. Developed a strong command of core computer science principles and modern software development practices. Professional and Personal Growth: Gained unparalleled insights into the tech industry and diverse career paths at Google. Established a network of mentors and peers, fostering long-term professional relationships and collaboration. Strategic Career Preparation: Acquired the knowledge and skills to excel in technical interviews and secure positions at top tech companies. Developed a clear roadmap for pursuing a successful and impactful career in technology. Participating in the Google Get Ahead program was a transformative journey, equipping me with the advanced technical skills, industry insights, and professional network necessary to propel my career aspirations in the technology sector to new heights.\n","permalink":"https://abirarsalane.github.io/experience/google/","summary":"Description Trainee at Google Get Ahead Program\nGoogle Get Ahead is an elite, invitation-only eight-week program tailored for exceptional female-identifying students eager to elevate their technical expertise and explore dynamic career opportunities at Google. The program offers a unique blend of immersive video training, challenging coding exercises, and invaluable access to Google’s top engineers.\nKey Responsibilities and Achievements: Intensive Video Trainings:\nAdvanced Technical Skill Development: Immersed myself in an extensive array of video trainings covering critical and advanced topics in computer science.","title":"Google Get Ahead Trainee"}]